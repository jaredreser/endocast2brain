# Endocast2Brain
Exploring AI methods to reconstruct brains of extinct animals from fossil endocasts.

Jared Edward Reser Ph.D.

Reason: Fossil brains don’t preserve, but endocasts hold hidden info. Let's isolate it.

Approach: Encoder–decoder models, multimodal embeddings, transfer learning.

Roadmap: Stage 1: proof of concept (human CT+MRI). Stage 2: cross-species generalization. Stage 3: apply to fossil hominins.

Status: Idea/proposal phase — no code yet. Future repo updates will include datasets, notebooks, and models.

Collaboration: I extend an open invitation to paleoneurologists, neuroscientists, and ML researchers to discuss or contribute.

References: https://www.observedimpulse.com/2025/09/how-ai-could-be-used-to-reconstruct.html



From Endocast to Brain: Using Encoder–Decoder Models for Fossil Neuroanatomy

I have been wondering recently about the brains of extinct animals. Unfortunately, the soft tissues of these brains almost always decay completely without any traces of fossilization. However, we do, of course, have the interior of the skull for many extinct animals from dinosaurs, to the first mammals, to our ancient ancestors. In fact, the inside of the skull is commonly preserved in fossils and its geometry can give researchers modest clues about its previous contents.

Scientists can use the hollowed-out brain cavity of the skull to make inferences about the brains of long-gone species such as the Tyrannosaurus rex and homo erectus. The shape of this cavity can tell us a lot about the T. rex brain, especially when we compare it to that of reptiles and birds. It tells us that the Tyrannosaurus had one of the largest brains of all the dinosaurs, and that it had large areas devoted to smell and sight. There are numerous informative details that can be gleaned from brain cases by trained paleontologists. But I suspect that many more details could be uncovered using machine learning.

In this essay, we will discuss how information could be squeezed out of brain endocasts. An endocast is a three-dimensional model or mold of the internal space of the cranial cavity of a skull. These casts are used in paleontology and paleoneurology to study the size, shape, and organization of the central nervous system of extinct animals. In this essay we will discuss how we might be able to take an endocast, say from an australopithecine, and make reliable guesses about its brain structure using AI. Of course, the interior of the skull is mostly smooth and thus the endocast does not contain gross brain anatomy, but I’m going to assume that it holds lots of hidden information that is invisible to humans, but that AI could discern.

The type of AI we would use would be a neural network like a 3D transformer model or 3D CNN. Such an AI system would be trained to look at an endocast and then predict what the corresponding brain should look like (generating plausible probabilistic brain anatomy). I believe that we can employ currently assessable (or obtainable) data to train such an AI system to do this. During training, we would have to show the system many matched pairs. For instance, one pair would be your brain and your endocast. Then we could use mine, and then many hundred more. The best way to do this might be to use a CT scan of the inside of the skull as a proxy for the endocast and then we could pair this with the MRI of the same brain. Perhaps 500 to 2,000 CT/MRI pairs would be needed.

We would have to give the AI hundreds of examples of these matched pairs to sufficiently train it to use the bone to predict the shape and form of the brain itself. Fortunately, much of this data has already been collected and exists in medical imaging datasets. Then we would collect similar data from apes and monkeys. Training on both humans and primates is crucial because it lets the system interpolate across evolutionary neighbors rather than just extrapolate. After training on this data, the system would be optimized to accept the endocast of a prehistoric human ancestor and produce a statistical 3D estimate of its brain. Cross-validation would come from leaving one species out of training and testing whether the model can predict its brain anatomy from the endocast alone. If it can reconstruct an orangutan brain without ever seeing one during training, for example, then it has captured a genuinely generalizable mapping.

This AI system would use cross-modal learning to analyze the relationships between two data types (bone geometry vs. brain anatomy). This would use a form of supervised learning where the matched pairs provide the system with a question (endocast) and the corresponding ground truth answer (brain anatomy) to check its predictions against. During training, when the AI system gets things wrong, we would weaken the weights responsible for making the mistake, and when it gets things right, we would strengthen them (backpropagation and gradient descent). Over time this feedback would optimize its ability to predict the brain by learning how the geometric features of the interior of skulls map onto brain anatomy. It would embed both skull interiors and brain anatomy in a shared mathematical space, learning the probabilistic mappings necessary to go from one to the other. It is important to remember that when the AI generates a brain, this wouldn’t represent a single “true” brain but a plausible reconstructions. Doing this many times would result in a range of possibilities that most likely capture many facets of the true brain. It could even do so while quantifying the uncertainty with confidence intervals.

To be analyzed by the computer the endocast would have to be turned into a number using morphometric analysis (i.e., 3D grid, point cloud, or mesh). Basically, the geometry of each inner skull would have to be mapped, parameterized and transformed into a long, standardized number (encoded) that can be read into a neural network so that it can be compared with other such numbers. This digitization process would also have to be done to the brains. The brains and endocasts would be trained to live in the same latent space via contrastive alignment.

We could train this system on humans and other primates so that we could ask it to triangulate toward hominins such as Neanderthals, Homo erectus, and Australopithecines. This technique would work for any species where we have a fossil skull interior and living relatives. However, the more ancient the animal, the less fidelity the system will have. Trying to similarly interpolate the brain of a Tyrannosaurus rex based on data from reptiles and birds would be much more difficult. This is because the evolutionary distances involve hundreds of millions of years rather than just a couple million years in the case of our prehistoric ancestors. Nevertheless, using this technique on animals that have been gone for eons could still produce meaningful results.

The AI would use an encoder–decoder architecture, where an encoder would turn the skull into a latent code, and a decoder would turn the code into a predicted brain. This prediction would be a conditional generation or probabilistic model. The trained model would use transfer learning (generalizing knowledge about humans and apes to hominins) to accomplish allometric domain adaptation. Essentially, I am proposing a form of geometric deep learning that uses morphometrics (the quantitative study of shape (volumes, curves, landmarks)) to process information about non-Euclidean data involving meshes and point clouds. This system trys to build a latent space where skull/endocast features and brain features line up creating correlational structure. It would not rely on a single topological feature, but rather integrate hundreds of subtle shape cues simultaneously (multimodal fusion) into nonlinear multivariate mappings. The system would embed both skulls and brains into a shared latent space, a kind of statistical arena where structural correspondences can be learned. Learning the brain/endocast correspondence essentially involves a geometry-to-geometry mapping.

I think a properly constructed machine learning system will be able to create copious information from the endocasts alone. I think it will be enough to make predictions about gross brain anatomy, and not just the brain’s surface (cortical mantle). Especially if thousands of human and primate brains can be compared to their endocasts. Even though they look smooth, endocasts contain multiple overlapping latent signals. Humans have trouble integrating these diffuse features simultaneously, but a neural network can. This will allow it to learn non-obvious mappings (e.g., that a particular vault curvature pattern predicts relative cerebellar size).

The endocast may be most informative for reconstructing the outer cortex that lies a few millimeters underneath it. But what about the deeper brain structures? I believe that by using full brains and detailed endocasts, the endocasts themselves might be able to offer plenty of information about the entire brain, including white matter and subcortex. Endocasts contain subcortical structures indirectly and this means that with enough training data, a network can map endocast geometry not just to the outer cortical sheet, but to whole-brain region proportions. Given sufficient compute and precision, I think it is possible that endocasts could even be used to make detailed predictions about connectivity or even fine-grained microanatomy. This could move paleoneurology from an “interpretive art” into statistical prediction of whole-brain anatomy. Using not just the endocast, but the entire skull could contribute additional informative data. It may even be possible to squeeze information about the brain out of the full skeleton.

Let’s talk about the landmarks on the endocast that the AI would have available to it to learn from. As you can see from the pictures there is a lot detail and keep in mind that there is a lot of variation in this detail from person to person. The folds of the cortex come in the form of gyri (ridges) and sulci (grooves) which can be visible in an endocast. But the level of detail depends heavily on the species, the individual, and the preservation quality. Smaller-brained primates, like macaques and australopithecines, tend to have more pronounced gyral and sulcal impressions on their endocasts than larger-brained hominins, including modern humans. In adult humans, the folds are barely visible, particularly on the upper part of the skull. The clarity of brain surface impressions on endocasts varies with age. Endocasts of developing brains in infants, for example, tend to show more detail than those of adults.

Despite limitations, paleoneurologists routinely use endocasts to study brain evolution in extinct species. They have successfully used the imprints of some major, consistently preserved sulci, such as the lunate sulcus, to track key evolutionary changes in hominin brain organization. However, the interpretation of these surface details remains a complex and sometimes subjective task, which is why using AI could be very helpful. Natural fossil endocasts, such as the famous Taung child (Australopithecus africanus), can have remarkably detailed surface features. For artificially or virtually created endocasts, the resolution of the imaging technique (e.g., CT scan) can dramatically influence the observable detail. New, high-resolution imaging and analysis techniques, though, are continuously improving the reliability of these analyses.

Meninges, blood vessels, and cerebrospinal fluid all exist between the brain and the bone above it and so they obscure cortical contact with bone. Nevertheless, consistent signals remain: endocranial volume, vault proportions, asymmetries, olfactory fossae, vascular channels, and gross lobe outlines. These are exactly the types of geometric data that machine learning excels at exploiting. The endocast can also give clues about the relative size and position of regions of the brain such as the frontal, temporal, parietal, and occipital lobes. However, the connection is weaker in the superior frontal and parietal areas.

There are a great number of measurable endocast traits, features, shapes, curves, and metrics, that can be extracted directly from the bony vault. These include:

Overall endocranial volume (ECV)
Vault shape and proportions (elongation, globularity)
Asymmetries (petalias)
Sulcal and gyral impressions (if present)
Vascular grooves
Cranial base angles and fossae
Foramen magnum orientation
Olfactory fossa and cribriform plate region
Canal openings
Cerebellar impressions
Brainstem/pons impressions
Relative lobe expansion (bulging and bossing, flattening and angulation)

What could the results of a system like this do for science? For fossils like the Taung child, AI could sharpen our sense of which sulci impressions are genuine. For Neanderthals, it could provide a statistical measure of parietal expansion. For dinosaurs, it might offer credible intervals for sensory specializations. It is worth mentioning that a machine learning model like the one discussed here could be used on much more recent skulls. It could even play a role in helping to model the brains of deceased humans. Having a recreation of a loved one’s brain could help make their AI counterpart or avatar more authentic. Realistically this kind of thing should probably only be done with permission, but I wouldn’t mind, and in fact, I would like to grant permission to use my skull and brain for machine learning right here and now.

By turning these cavities into data-rich predictors, AI could breathe new life into the study of extinct cognition and allow us to glimpse the hidden architecture of minds long vanished. While the results will always be probabilistic and uncertain, they could bring new rigor to paleoneurology, transforming smooth stone endocasts into testable models of ancient cognition. The smooth surfaces of fossil skulls, long thought to be mute, may hold hidden records that only modern computation can translate. In doing so, we may begin to see not only the brains, but also the minds, from worlds long vanished.
